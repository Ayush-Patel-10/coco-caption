{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1shiV3pBnnLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa79988b-d5b5-45d7-eadb-d5452edc3303"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmxKxmoeit7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6061528c-6085-430b-fe4f-f47c348637df"
      },
      "source": [
        "! git clone https://github.com/LuoweiZhou/coco-caption"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'coco-caption'...\n",
            "remote: Enumerating objects: 790, done.\u001b[K\n",
            "remote: Total 790 (delta 0), reused 0 (delta 0), pack-reused 790\u001b[K\n",
            "Receiving objects: 100% (790/790), 128.16 MiB | 21.88 MiB/s, done.\n",
            "Resolving deltas: 100% (455/455), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKnFB6kdi4HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4626cd9-cbae-462b-dfe0-900fc89692f6"
      },
      "source": [
        "%cd /content/coco-caption/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/coco-caption\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "\n",
        "with zipfile.ZipFile('/content/coco-caption.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipdir('/content/coco-caption', zipf)\n"
      ],
      "metadata": {
        "id": "fb-L-gkAe6Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaBVhnRqir0K"
      },
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "#from pycocoevalcap.eval import COCOEvalCap\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "import json\n",
        "from json import encoder\n",
        "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMtHeqnrir0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e0871c-4175-4230-8324-759acb0f1b8a"
      },
      "source": [
        "# set up file names and pathes\n",
        "dataDir='.'\n",
        "dataType='val2014'\n",
        "algName = 'fakecap'\n",
        "annFile='%s/annotations/captions_%s.json'%(dataDir,dataType)\n",
        "subtypes=['results', 'evalImgs', 'eval']\n",
        "[resFile, evalImgsFile, evalFile]= \\\n",
        "['%s/results/captions_%s_%s_%s.json'%(dataDir,dataType,algName,subtype) for subtype in subtypes]\n",
        "\n",
        "# download Stanford models\n",
        "!./get_stanford_models.sh"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "--2023-11-29 04:56:03--  http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip [following]\n",
            "--2023-11-29 04:56:03--  https://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2015-12-09.zip [following]\n",
            "--2023-11-29 04:56:03--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2015-12-09.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 403157240 (384M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2015-12-09.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 384.48M  5.01MB/s    in 72s     \n",
            "\n",
            "2023-11-29 04:57:15 (5.36 MB/s) - ‘stanford-corenlp-full-2015-12-09.zip’ saved [403157240/403157240]\n",
            "\n",
            "Unzipping...\n",
            "Archive:  stanford-corenlp-full-2015-12-09.zip\n",
            "   creating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/\n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/xom-1.2.10-src.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/CoreNLP-to-HTML.xsl  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/README.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/LIBRARY-LICENSES  \n",
            "   creating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/sutime/\n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/sutime/defs.sutime.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/sutime/english.sutime.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/sutime/english.holidays.sutime.txt  \n",
            " extracting: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/ejml-0.23-src.zip  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/input.txt.xml  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/build.xml  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/pom.xml  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0.jar  \n",
            "   creating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/tokensregex/\n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/tokensregex/color.input.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/tokensregex/retokenize.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/tokensregex/color.properties  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/tokensregex/color.rules.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/javax.json-api-1.0-sources.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/protobuf.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/StanfordDependenciesManual.pdf  \n",
            "   creating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/\n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/example.properties  \n",
            " extracting: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/otherpeople.txt  \n",
            " extracting: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/goldplaces.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/stopwords.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/presidents.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/names.txt  \n",
            " extracting: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/places.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/patterns/goldnames.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/slf4j-simple.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/input.txt  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/input.txt.out  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/joda-time.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/xom.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/StanfordCoreNlpDemo.java  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-sources.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/slf4j-api.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/jollyday-0.4.7-sources.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/ejml-0.23.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/javax.json.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/Makefile  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-models.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/corenlp.sh  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/joda-time-2.9-sources.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-javadoc.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/jollyday.jar  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/ShiftReduceDemo.java  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/SemgrexDemo.java  \n",
            "  inflating: pycocoevalcap/spice/lib/stanford-corenlp-full-2015-12-09/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAkuY4_uir07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31eb524-cca7-47d1-8731-d934d31de906"
      },
      "source": [
        "# create coco object and cocoRes object\n",
        "coco = COCO(annFile)\n",
        "cocoRes = coco.loadRes(resFile)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "0:00:00.477948\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxpGz12jdyeU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usf2KWo47601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1f6e1e-efc5-4f02-c1f2-b5db64fc5972"
      },
      "source": [
        "#__author__ = 'tylin'\n",
        "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "\n",
        "class COCOEvalCap:\n",
        "    def __init__(self, coco, cocoRes):\n",
        "        print('ullu')\n",
        "        self.evalImgs = []\n",
        "        self.eval = {}\n",
        "        self.imgToEval = {}\n",
        "        self.coco = coco\n",
        "        self.cocoRes = cocoRes\n",
        "        self.params = {'image_id': coco.getImgIds()}\n",
        "\n",
        "    def evaluate2(self, gts, res):\n",
        "\n",
        "        print('tokenizationme...')\n",
        "\n",
        "        tokenizer = PTBTokenizer()\n",
        "        gts  = tokenizer.tokenize(gts)\n",
        "        res = tokenizer.tokenize(res)\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "            (Meteor(),\"METEOR\"),\n",
        "            (Rouge(), \"ROUGE_L\"),\n",
        "            (Cider(), \"CIDEr\"),\n",
        "            (Spice(), \"SPICE\")\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(gts, res)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def evaluate(self):\n",
        "        print(\"sdfsadf\")\n",
        "        print('d')\n",
        "        imgIds = self.params['image_id']\n",
        "        # imgIds = self.coco.getImgIds()\n",
        "        gts = {}\n",
        "        res = {}\n",
        "        a=0\n",
        "        for imgId in imgIds:\n",
        "            gts[imgId] = self.coco.imgToAnns[imgId]\n",
        "            res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
        "            if(a<1):\n",
        "              print('')\n",
        "              print(self.coco.imgToAnns[imgId])\n",
        "              print(self.cocoRes.imgToAnns[imgId])\n",
        "              a+=1\n",
        "              print('')\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenizationme...')\n",
        "\n",
        "        tokenizer = PTBTokenizer()\n",
        "        gts  = tokenizer.tokenize(gts)\n",
        "        res = tokenizer.tokenize(res)\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "            (Meteor(),\"METEOR\"),\n",
        "            (Rouge(), \"ROUGE_L\"),\n",
        "            (Cider(), \"CIDEr\"),\n",
        "            (Spice(), \"SPICE\")\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(gts, res)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = {}\n",
        "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-PFekEOzEF7"
      },
      "source": [
        "# resutls on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuaRzGvaZhX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed64866d-af38-4e41-90a3-7340248e682b"
      },
      "source": [
        "with open('/content/drive/My Drive/Image-captioning-Researc-paper-implimentation-master/predictedCaption.json', 'r') as file:\n",
        "  jfile = json.load(file)\n",
        "with open('/content/drive/My Drive/Image-captioning-Researc-paper-implimentation-master/annotation.json', 'r') as file:\n",
        "  cfile = json.load(file)\n",
        "\n",
        "print(\"Files loaded\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRBWcRBrZjh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65680a98-526c-4641-9b95-8163f67c4c3c"
      },
      "source": [
        "gts={}\n",
        "res = {}\n",
        "\n",
        "for i in range(len(jfile)):\n",
        "\n",
        "  tempResList = []\n",
        "\n",
        "  image_id=jfile[i]['image_id']\n",
        "  caption = jfile[i]['caption']\n",
        "  dict1 = {'image_id' : image_id, 'id' : i, 'caption' : caption}\n",
        "  tempResList.append(dict1)\n",
        "  res.update({image_id: tempResList})\n",
        "\n",
        "  tempGts = []\n",
        "  for dicta in cfile:\n",
        "    if(int(dicta['id']) == image_id):\n",
        "      dict2 = {'image_id': image_id, 'id':i, 'caption': dicta['caption']}\n",
        "      tempGts.append(dict2)\n",
        "  gts.update({image_id: tempGts})\n",
        "  if(i%100==0):\n",
        "    print(i)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff-zA8nOMAmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afc9b40-b16f-41b8-c21c-1f79bd422f1f"
      },
      "source": [
        "len(res), len(gts)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4867, 4867)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CScFL0Y02lEZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "6c477d81-e339-4746-b676-0615c09716e9"
      },
      "source": [
        "# create cocoEval object by taking coco and cocoRes\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "\n",
        "# evaluate on a subset of images by setting\n",
        "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "# please remove this line when evaluating the full validation set\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "\n",
        "# evaluate results\n",
        "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
        "cocoEval.evaluate2(gts, res)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ullu\n",
            "tokenizationme...\n",
            "setting up scorers...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-feb35e1ae313>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# SPICE will take a few minutes the first time, but speeds up due to caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-fafa67cf619e>\u001b[0m in \u001b[0;36mevaluate2\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mMeteor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"METEOR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mRouge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROUGE_L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mCider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CIDEr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mSpice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SPICE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         ]\n",
            "\u001b[0;31mTypeError\u001b[0m: Cider.__init__() missing 1 required positional argument: 'df'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1diYE5PCir1b"
      },
      "source": [
        "# print output evaluation scores\n",
        "for metric, score in cocoEval.eval.items():\n",
        "    print('%s: %.2f'%(metric, score*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7U1R6vsir1m"
      },
      "source": [
        "# demo how to use evalImgs to retrieve low score result\n",
        "evals = [eva for eva in cocoEval.evalImgs if eva['CIDEr']<30]\n",
        "print('ground truth captions')\n",
        "imgId = evals[0]['image_id']\n",
        "annIds = coco.getAnnIds(imgIds=imgId)\n",
        "anns = coco.loadAnns(annIds)\n",
        "coco.showAnns(anns)\n",
        "\n",
        "print('\\n')\n",
        "print('generated caption (CIDEr score %0.1f)'%(evals[0]['CIDEr']))\n",
        "annIds = cocoRes.getAnnIds(imgIds=imgId)\n",
        "anns = cocoRes.loadAnns(annIds)\n",
        "coco.showAnns(anns)\n",
        "\n",
        "img = coco.loadImgs(imgId)[0]\n",
        "I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
        "plt.imshow(I)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui2RXfB5ir10"
      },
      "source": [
        "# plot score histogram\n",
        "ciderScores = [eva['CIDEr'] for eva in cocoEval.evalImgs]\n",
        "plt.hist(ciderScores)\n",
        "plt.title('Histogram of CIDEr Scores', fontsize=20)\n",
        "plt.xlabel('CIDEr score', fontsize=20)\n",
        "plt.ylabel('result counts', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOzPiqanir19"
      },
      "source": [
        "# save evaluation results to ./results folder\n",
        "json.dump(cocoEval.evalImgs, open(evalImgsFile, 'w'))\n",
        "json.dump(cocoEval.eval,     open(evalFile, 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxldtHws9o2w"
      },
      "source": [
        "184321\n",
        "184321\n",
        "81922\n",
        "81922\n",
        "577539\n",
        "577539\n",
        "380932\n",
        "380932\n",
        "204805\n",
        "204805\n",
        "339974\n",
        "339974\n",
        "153607\n",
        "153607\n",
        "440329\n",
        "440329\n",
        "310325\n",
        "310325\n",
        "501762\n",
        "501762\n",
        "419856\n",
        "419856\n",
        "1369\n",
        "1369\n",
        "234500\n",
        "234500\n",
        "241691"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN4qNFWiio76"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}